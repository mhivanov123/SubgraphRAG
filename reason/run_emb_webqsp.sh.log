Loading nccl/2.23.4-cuda12.6
  Loading requirement: cuda/12.6
starting experiment
WARNING 01-06 12:44:15 config.py:1563] Casting torch.bfloat16 to torch.float16.
WARNING 01-06 12:44:15 arg_utils.py:849] The model has a long context length (85000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 12:44:15 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='/home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=85000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)
/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/executor/gpu_executor.py:49: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 01-06 12:44:22 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 01-06 12:44:22 selector.py:116] Using XFormers backend.
/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
INFO 01-06 12:44:33 model_runner.py:879] Starting to load model /home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct...
INFO 01-06 12:44:42 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 01-06 12:44:42 selector.py:116] Using XFormers backend.
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [01:07<01:07, 67.63s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:26<00:00, 39.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:26<00:00, 43.30s/it]

INFO 01-06 12:46:10 model_runner.py:890] Loading model weights took 6.0160 GB
INFO 01-06 12:46:38 gpu_executor.py:121] # GPU blocks: 0, # CPU blocks: 2340
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/gridsan/mhadjiivanov/meng/SubgraphRAG/reason/main.py", line 208, in <module>
[rank0]:     main()
[rank0]:   File "/home/gridsan/mhadjiivanov/meng/SubgraphRAG/reason/main.py", line 160, in main
[rank0]:     llm = llm_init(model_name, tensor_parallel_size, max_seq_len_to_capture, max_tokens, seed, temperature, frequency_penalty)
[rank0]:   File "/home/gridsan/mhadjiivanov/meng/SubgraphRAG/reason/llm_utils.py", line 13, in llm_init
[rank0]:     client = LLM(model=model_name, tensor_parallel_size=tensor_parallel_size, max_seq_len_to_capture=max_seq_len_to_capture, dtype="half", enable_chunked_prefill=False, max_model_len=85000)
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/entrypoints/llm.py", line 175, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 473, in from_engine_args
[rank0]:     engine = cls(
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 284, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 403, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/executor/gpu_executor.py", line 124, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/worker/worker.py", line 257, in initialize_cache
[rank0]:     raise_if_cache_size_invalid(num_gpu_blocks,
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/worker/worker.py", line 471, in raise_if_cache_size_invalid
[rank0]:     raise ValueError("No available memory for the cache blocks. "
[rank0]: ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.
WARNING 01-06 12:47:04 config.py:1563] Casting torch.bfloat16 to torch.float16.
WARNING 01-06 12:47:04 arg_utils.py:849] The model has a long context length (85000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 12:47:04 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='/home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=85000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)
/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/executor/gpu_executor.py:49: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 01-06 12:47:04 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 01-06 12:47:04 selector.py:116] Using XFormers backend.
/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
INFO 01-06 12:47:05 model_runner.py:879] Starting to load model /home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct...
INFO 01-06 12:47:05 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 01-06 12:47:05 selector.py:116] Using XFormers backend.
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.50s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.02s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.24s/it]

INFO 01-06 12:47:10 model_runner.py:890] Loading model weights took 6.0160 GB
INFO 01-06 12:47:29 gpu_executor.py:121] # GPU blocks: 0, # CPU blocks: 2340
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/gridsan/mhadjiivanov/meng/SubgraphRAG/reason/main.py", line 208, in <module>
[rank0]:     main()
[rank0]:   File "/home/gridsan/mhadjiivanov/meng/SubgraphRAG/reason/main.py", line 160, in main
[rank0]:     llm = llm_init(model_name, tensor_parallel_size, max_seq_len_to_capture, max_tokens, seed, temperature, frequency_penalty)
[rank0]:   File "/home/gridsan/mhadjiivanov/meng/SubgraphRAG/reason/llm_utils.py", line 13, in llm_init
[rank0]:     client = LLM(model=model_name, tensor_parallel_size=tensor_parallel_size, max_seq_len_to_capture=max_seq_len_to_capture, dtype="half", enable_chunked_prefill=False, max_model_len=85000)
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/entrypoints/llm.py", line 175, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 473, in from_engine_args
[rank0]:     engine = cls(
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 284, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 403, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/executor/gpu_executor.py", line 124, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/worker/worker.py", line 257, in initialize_cache
[rank0]:     raise_if_cache_size_invalid(num_gpu_blocks,
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/worker/worker.py", line 471, in raise_if_cache_size_invalid
[rank0]:     raise ValueError("No available memory for the cache blocks. "
[rank0]: ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.
WARNING 01-06 12:47:39 config.py:1563] Casting torch.bfloat16 to torch.float16.
WARNING 01-06 12:47:39 arg_utils.py:849] The model has a long context length (85000). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 01-06 12:47:39 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='/home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='/home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=85000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)
/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/executor/gpu_executor.py:49: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 01-06 12:47:40 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 01-06 12:47:40 selector.py:116] Using XFormers backend.
/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
INFO 01-06 12:47:40 model_runner.py:879] Starting to load model /home/gridsan/mhadjiivanov/meng/SubgraphRAG/hf/models/Llama-3.2-3B-Instruct...
INFO 01-06 12:47:40 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 01-06 12:47:40 selector.py:116] Using XFormers backend.
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.97s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.75s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.94s/it]

INFO 01-06 12:47:44 model_runner.py:890] Loading model weights took 6.0160 GB
INFO 01-06 12:48:04 gpu_executor.py:121] # GPU blocks: 4428, # CPU blocks: 2340
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/gridsan/mhadjiivanov/meng/SubgraphRAG/reason/main.py", line 208, in <module>
[rank0]:     main()
[rank0]:   File "/home/gridsan/mhadjiivanov/meng/SubgraphRAG/reason/main.py", line 160, in main
[rank0]:     llm = llm_init(model_name, tensor_parallel_size, max_seq_len_to_capture, max_tokens, seed, temperature, frequency_penalty)
[rank0]:   File "/home/gridsan/mhadjiivanov/meng/SubgraphRAG/reason/llm_utils.py", line 13, in llm_init
[rank0]:     client = LLM(model=model_name, tensor_parallel_size=tensor_parallel_size, max_seq_len_to_capture=max_seq_len_to_capture, dtype="half", enable_chunked_prefill=False, max_model_len=85000)
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/entrypoints/llm.py", line 175, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 473, in from_engine_args
[rank0]:     engine = cls(
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 284, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 403, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/executor/gpu_executor.py", line 124, in initialize_cache
[rank0]:     self.driver_worker.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/worker/worker.py", line 257, in initialize_cache
[rank0]:     raise_if_cache_size_invalid(num_gpu_blocks,
[rank0]:   File "/home/gridsan/mhadjiivanov/.local/lib/python3.9/site-packages/vllm/worker/worker.py", line 476, in raise_if_cache_size_invalid
[rank0]:     raise ValueError(
[rank0]: ValueError: The model's max seq len (85000) is larger than the maximum number of tokens that can be stored in KV cache (70848). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
done
